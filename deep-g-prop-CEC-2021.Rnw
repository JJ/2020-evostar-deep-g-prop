\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{hyperref}

<<setup, cache=FALSE,echo=FALSE,message=FALSE,warning=FALSE>>=
require(ggplot2)
require(ggthemes)
@

\begin{document}
%
\title{EvoMLP: a framework for evolving multilayer perceptrons}
%
\author{\IEEEauthorblockN{Luis Liñán-Villafranca}
  \IEEEauthorblockA{\textit{RTI} \\
    {\tt luis@rti.com}}
  \and
  \IEEEauthorblockN{Mario Garc\'ia-Valdez}
  \IEEEauthorblockA{\textit{Instituto Tecnol\'ogico de Tijuana} \\
    {\tt mario@tectijuana.edu.mx}}
  \and
  \IEEEauthorblockA{J.J. Merelo, Pedro Castillo-Valdivieso}
  \IEEEauthorblockN{Computer Architecture and Technology\\
    University of Granada, Spain\\
    {\tt {jmerelo|pacv}@ugr.es}}
}
%
\maketitle              % typeset the header of the contribution

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Designing neural networks for classification or regresion can be
considered a search problem, and, as such, can be approached using
different optimization procedures. There are also several design
challenges. The first and more important is to constrain the search
space in such a way that proper solutions can be found in a reasonable
amount of time; the second is to take into account that, depending on
how the optimization procedure is formulated, the fitness score used
for it can have a certain degree of uncertainty. This means that
creating a framework for evolving neural networks for classification
implies taking a series of decisions that range from the purely
technical to the algorithmical at different levels: neural or the
optimization framework chosen. This will be the focus of this paper,
where we will introduce DeepGProp, a framework for genetic
optimization of multilayer perceptrons, that is able to explore the
space of neural nets with different layers and layer size and find
good solutions in a reasonable amount of time. The different design
decisions taken and how they affect the output will be presented and
compared with other state of the art frameworks.

\end{abstract}

\begin{IEEEkeywords}
  Neuroevolution, Python, Backpropagation, multilayer perceptrons,
  software frameworks, MLPs.
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Applying artificial neural networks (ANN) to the solution of
classification problems requires establishing their structure in
layers and connections between them, the initial parameters (such as
initial weights) and a set of learning constants. 
Using an optimization algorithm to solve at least part of this design problem
is an usual approach to this challenge, and evolutionary neural networks \cite{GOMESPEREIRADELACERDA2021100777,Martinez2021LightsAS} have been proved to be an efficient way of searching for the
architecture, weights, and other ANN parameters (such as learning
constants). This has been proved
repeatedly for {\em shallow} architectures (with a single hidden
layer) since late in the previous century
\cite{yao1993evolutionary,CastilloNPL,stanley2002evolving}. However, in most cases, design
of the structure and connections between different layers, as
indicated by \cite{miikkulainen2019evolving}, is still mostly done by
hand and left off the evolutionary process. This is mainly due to the above mentioned fact that including this makes search
spaces huge, so using rules of thumb (such as the ones mentioned
in \cite{qolomany2017parameters}, or using certain formulas to set the
number of layers) to decide on those parameters,
are a way of fixing part of the search space, letting the neural net training algorithm to figure out the weights themselves, whose
optimization, in these cases, is able to cover big spans of the search
space. Some of these algorithms stop short of setting weights, and leave that to an actual neural net training algorithm. The fitness of the neural net is measured {\em after} training, thus including training time within the fitness evaluation time. Adding long evaluation time to the size of the search space makes, sometimes, the mixture of evolutionary algorithms with neural networks something that can be approached only with big evaluation budgets.

That high budget needed needs to be managed, and this is done in several ways, usually. One of them is to fix a part of the architecture, by choosing a
multi-layer perceptron (MLPs) using some well-proven training algorithm. After all, MLPs with a single hidden layer are universal approximators, and have been used extensively for classification and regression problems. Once that part of the search space is fixed, the initial
weights as well as biases will evolve using an evolutionary algorithm (EA), and some MLP training algorithm such as backpropagation will be used to find the final MLP that's tested and eventually produced as the solution to a problem. This algorithm was proposed in the early years of the century, and called \emph{G-Prop} (as in \emph{Genetic Back-Propagation}), originally presented in \cite{castilloNC,CastilloNPL}. G-Prop evolves a population of data structures representing multi-layer perceptrons initial weights and biases, which are trained and tested to obtain the fitness. 
This method leverages the capabilities of two classes of algorithms: the
ability of EA to find a solution close to the global optimum, and the
ability of the back-propagation algorithm (BP) \cite{Rumelhart86} to tune a solution and
reach the nearest local minimum by means of local search from the
solution found by the EA.

This initial G-Prop algorithm was limited, first, by the fact that
most of it had to be programmed from scratch, since there were few (if
any) off-the-shelf software that could do this kind of task;
additionally, available computational capability limited the size of
the search space; this was further limited by the evolution of only single-layer perceptrons. This is why only small problems could be approached, and the evaluation budget used in them was, forcibly, limited.

However, the {\em memetic} combination of optimization at two levels, the weight level (handled by backpropagation) and the parametrized architecture level (handled by an evolutionary algorithm), is still conceptually new, since not many methods use them. Besides, the implementation, even being open source, is not maintained. This is why in this paper we propose, first, a reimplementation of the G-Prop concept, and second, an extension that solves some problems with this initial method: the inability to work with neural nets with several layers, and also the implicit dealing with the uncertainty in computing the fitness using a stochastic method like neural net training.

Thus, the aim of this paper is to present EvoMLP, a neuroevolution framework that evolves the initial weights, number of
hidden layers and hidden layer sizes of a MLP, based on a EA and
Stochastic Gradient Descent (SGD) \cite{bottou2012stochastic}. This
algorithm has been used for training deep neural nets and other kind
of machine learning algorithms
\cite{bottou2012stochastic,bottou2010large}; however, in this case
intermediate (or hidden) layers have the same structure,
which is the structure of a multi-layer perceptron, justifying our
denomination of {\sf EvoMLP} for this specific method.

In this paper what we will do is to first bring the implementation of
the G-Prop concept to current tools and languages, choosing the best
suited for the purpose and show the design decisions that were taken in order to do so; since we are using off the shelf software libraries which are in constant evolution, we will also show, since implementation matters \cite{DBLP:conf/iwann/MereloRACML11}, how this change of versions affects (or not) the performance of the algorithm. We will also attempt to characterize the size of the problems for which this type of algorithm can be considered useful.

The remainder of this paper is structured as follows:
section \ref{soa} makes a short overview of the methods to design ANN.
In section \ref{method} it is fully described the proposed model.
Section \ref{sec:res} describes the results obtained,
followed by a brief conclusion in Section \ref{sec:conclus}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{State of the Art}
\label{soa}

Searching the parameter space of neural networks looking for optimal combinations  has been a problem traditionally approached
in a limited number of ways, using {\em incremental algorithms}, which, in
general, will depart from a specific architecture, number of layers
and elements in every layer, and offer a series of heuristic
procedures to add/eliminate layers and/or hidden-layer neurons in
% I think at this time there are only input and output layers (and these must remain fixed) 
% and we have the option of adding and removing only hidden layers. 
% I propose to only say: to add/eliminate layers and the number of neurons in them 
% In more advanced networks there are other types like  recurrent, convolution etc. 
% but making the distinction is a bit confusing - M 
% Can you please ellaborate on this? Or simply rewrite it, no problem - JJ
them. This way was, for a certain amount of time, the preferred technique
for optimizing the size of the neural net along with its weights, according to classical reviews such % architecture of the nn ? - M
% don't get it, sorry - JJ
as the one by Alpaydin \cite{alpaydin1994gal}. However and simultaneously, a
series of evolutionary heuristics were starting to be applied for the
same purpose, as revealed by Balakrishnan and Honavar's review
\cite{balakrishnan95:EDNA}. They propose a taxonomy, that classifies evolutionary
design of the architecture of neural nets along three different axes
(plus one for application domain): genotype representation, network
topology, and a third axis that includes basically everything else,
``variables of evolution''.
This classification was much more comprehensive, and included many more degrees of freedom in evolution than simply adding or eliminating neurons. However, it still left some constraints and probably did not emphasize the parameters that are most likely to have the greater influence in the outcome.

This is why, form our point of view, a more precise classification of evolved neural architectures would include: \begin{itemize}
  \item Fixed or variable architecture. Irrespective of the kind of
    data structured used to represent it, what is relevant is if that
    representation allows for a variation of the architectural
    parameters, that is, adding or eliminating new nodes, layers, or
    in general, nodes and edges in a graph representation of the
    architecture.
  \item Articulation of the neural learning method. There is a whole
    range of possibilities, from letting the evolutionary process set
    all weights and other parameters, to using a learning process just
    to find the success rate, with additional possibilities like
    evaluating success {\em before applying learning} (to take
    advantage of the Baldwin effect \cite{castillo-2006}) to applying
    a few cycles of learning that will become part of the codified
    solution.
  \item How the lack of a "crisp" value for the fitness, is taken into account
    during evolution. Either from the fact that an stochastic algorithm is
    used to train the neural net before evaluating fitness, or the
    fact that the data set (or how it's presented for training) might not be totally fixed but subject to
    randomness, the fitness of a neural net cannot be pinned down to a
    single, crisp, number. This randomness in the fitness can be taken
    into account during the selection phase
    \cite{DBLP:conf/ijcci/MereloLFGCCRMG15}, or not. This is related to 
    having models that can generalize better when classifying unseen data,
    reducing overfitting. 
    % Please check or extend the above sentence - M
    % There is also the problem of overfitting - M
    % It's OK - JJ
  \item Mono or multiobjective optimization. In general, the main issue is to get the highest accuracy possible. However, 
  several other factors can be taken into account; in unbalanced data sets, accuracy for every one of the categories must be taken into account separately; besides, it is almost impossible to optimize by generalization, so a proxy for it, size, is instead optimized. All these objectives can be aggregated into a single one (using weights), hierarchized or considered properly as different objectives to be optimized separately.
 % Single or multiobjective?
 % There is more research in this aspect, changing "size" by "entropy", "message length" etc., there
 % is much more future work to do, just in this aspect alone. - M 
 % In the area of decision tree classifiers there are many estimators of their size/complexity.

\end{itemize}

With these axes in mind, the first one, the variation of architecture, has been the one that has developed the most,  lately evolving towards (almost)
unconstrained evolution using Neuroevolution of Augmenting Topologies (NEAT), as shown in the recent review by
Miikkulainen et al. \cite{miikkulainen2019evolving}. Since this blows
up the search space (essentially evolving graphs), these methods need computational resources that are
really beyond the reach of most labs or individuals. This is the reason why most of the latest publications 
still use a fixed model for the network architecture, for instance SVM % support vector machines? these are not NN,
% or is it a kind of network I don't know of? - M
or MLPs, and even, in some cases, fixed number of layers and weights
\cite{ecer2020training}; however, recent papers like the one by
Tajmiri et al. \cite{TAJMIRI2020108997} already use a representation
that is able to accommodate a maximum number of layers, as well as the
number of neurons in every layer. This encoding of
architecture does not have any place for initial weights, which in the
case of G-Prop, were proved to be essential for a good performance,
including the emergence of a Baldwin effect \cite{castillo-2006}
during evolution.
On the other hand, lately multi-objective algorithms are being explored. Senhaji et al. \cite{SENHAJI20201} optimizes performance at the same time that minimizes size using the NSGA-II algorithm. This is a generally good strategy, once again involving, as is usual in multiobjective algorithms, a boost in the size of the search space requiring big evaluation budgets.

More generally, network architecture search algorithms are also model search algorithms \cite{mazzawi21}, a framework for which was recently released by Google. These methods are generally appropriate if you have unlimited evaluation budget or if problems are complex enough that only this kind of algorithms are able to find the correct model.

However, there is a huge amount of moderately-sized problems (with their corresponding datasets), where constrained search can be successful.
In this paper we will extend the principles of variation of a MLP
architecture through the use of genetic operators as well as
codification of initial weights to a modern, standard based
implementation, that will also be released as free software. We show
how this new {\sf EvoMLP} method works next. This means it will again use the architecture axis of evolution, although constraining it to a particular type of neural nets so that, by constraining search, we shrink the search space and make finding good architectures easier. Even so, the size of the datasets (and thus problems) that can be approached must be characterized, which we will do in the next sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Method}
\label{method}

There are two parts in this EvoMLP framework, the "neural" part and the "evolutionary" part. Our objective from the beginning was to use off-the-shelf frameworks as much as possible.

The first line of choice is the programming language. {\sf G-Prop} was written in C++ mainly because the underlying library used, EO \cite{EO:FEA2000}, used that language. In this case, we went in the opposite direction. Most popular high-performance machine learning libraries, like Tensorflow, are written in Python. Once we settled for TensorFlow, working with Python was the obvious choice.

There are indeed several evolutionary frameworks written in Python; EvoloPy, for instance \cite{DBLP:conf/ijcci/FarisAMCM16}, includes many hyperheuristics, including EAs. However, DEAP \cite{deap-ga} has emerged as a standard for this language, covering all that is needed to evolver multilayer perceptrons, so this is what we chose. This will definitely have some impact in performance \cite{kim2019software}, however for us researcher time is more important than computer time, and we can always use that time saved to improve the algorithm or the implementation.

Tensorflow is a low-level framework based on representing machine learning models as graphs; we will need a higher-level library to work with multilayer perceptrons. Once again, unlike G-Prop, we will choose the off-the-shelf Keras package \cite{keras-nn}, a framework for building multi-layer deep learning models In the version used in the initial stages of this research, Keras allowed multiple backends, meaning different frameworks for the effective representation of the neural net. That has changed lately, as we will later see. Once again, Keras is characterized for offering a simple API as opposed to a flexible and low-level interface, which is characteristic of Tensorflow or others like PyTorch; this has an impact on performance, but it helps ease the burden of the designers. The fact that it's maintained and updated often also ensures that the software created with it is kept up-to-date. This is important for a software framework like the one we're presenting here.

We need to make a series of decisions on the data structures and functions used in Keras; these are presented below. An individual is coded as a list of \emph{Layers}. Each layer is
    composed by a matrix of weights and a list of bias (both Numpy
    \cite{py-numpy} ndarrays) joined together with it's configuration (a
    Python dictionary) in a Python class. One column of the weight matrix
    represents all the connections from the previous layer to the nth neuron.
    All the weights and bias are randomly initialized with a uniform
    distribution between -1.0 and 1.0. A random proportion of all this genes
    and biases are muted for all the individuals each generation.

    From this configuration, when the moment of evaluation comes, we create a
    \emph{Sequential} \cite{keras-sequential}  Keras  model and
    append each \emph{Layer} as a \emph{Dense} \cite{keras-dense} Keras layer.

There are three stop conditions that will finish the execution of the
    genetic algorithm: if we reach the maximum number of generations, if after
    10 generations there was no improvement in the best solution and if we find
    the ideal solution ($0\%$ error, a single neuron left and 1 on the F2
    score). We used these 10 generations as an indication that evolution was stuck and could proceed no further, thus stopping being a better option.

For each generation we clone the best half individuals of the current
    population and perform all the evolutionary operations to them. Each
    operation has it own probability to occur chosen when running EvoMLP
    CLI. For the weights/bias mutation, a percentage of each (selected too with
    the CLI parameters) are muted randomly. After that, the worst half is
    replaced by this muted group.

The {\em crossover} function will swap the ``neurons'' together with
    their connections. Main difference with respect to original G-Prop is that,
    in this case, there can be several hidden layers, so the connections might
    be either to output or to another layers. The crossover only occurs when
    the two models selected for it have the same structure (same number of
    layers and neurons per layers).
    
    % For future work: the crossover could select structure of one of the parents or mix-them. - M
    % I don't know what you mean here, maybe you could directly add it to the future work section - JJ
    
There are several kinds of {\em mutation} operators: (initial) weights,
    biases, as well as changing the number of neurons and/or
    layers. For the neuron mutator, one neuron will be added last in the
    randomly selected layer, and it will be eliminated in the same way. Since
    we use fully connected neural networks, it does not really matter where the
    neuron is added or eliminated. The layer add/eliminate mutator is the
    main difference with respect to G-Prop, which used only one
    layer. In this case, only the last hidden layer can be eliminated. When a
    layer is eliminated, the weights from the previous layer to the eliminated
    one are kept, adding randomly generated weights and biases if the output
    layer (newly connected with this previous layer) have more ``neurons'' than
    the recently deleted layer. When adding a new hidden layer, the connections
    between the last hidden layer and the output layer are recycled, and the
    connections between the newly created layer and the output layer are
    calculated randomly (running similar fix as when we delete the layer).

We need the selection process, via the fitness function, to exert parsimony pressure, which is why
we take into account not only the accuracy score reached after
training, but also the overall size of the network using this score: 
\begin{equation}
f_{s} = (\sum_{i=0}^{nhl} nu_{i}) \times nhl
\end{equation}
where $f_{s}$, is the hidden-layer, or size, score, $nhl$ is the number of hidden layers and $nu_{i}$ the number of neurons for the layer $i$). This score is minimized, meaning that the less neurons, the better, but also that less hidden layers are preferred over more hidden layers. This $f_s$, however, receives a very low weight equivalent to 1\% of the accuracy, meaning that it will be only determinant, in practice, if there's a very small difference in accuracy when comparing two individuals of if accuracy is virtually the same.

Finally, the F2 score metric (explained in \autoref{sec:res}) is also considered. At the end, the fitness value is a compound of
three elements: accuracy, hidden-score and F2 score. As the DEAP framework
\cite{deap-ga} was used as a base to create the genetic algorithm, we also
took advantage of its fitness class \cite{deap-fitness} to rank the individuals
of a population. It performs a lexicographical comparison of the three values,
so we chose the order previously mentioned to approach as much as possible to
the fitness function used by G-Prop.

We handle uncertainty by {\em resampling}, that is, re-evaluating all individuals every generation. Every training-testing set will yield a different fitness, which usually have a skewed distribution \cite{DBLP:conf/ijcci/MereloLFGCCRMG15}. However, by repeatedly evaluating them individuals that have a statistically better fitness that the rest will {\em survive} to the next generation. This method, though simple, is generally adequate and tends to select individuals whose fitness distribution has a lower standard deviation and are less skewed.

The library was packaged and uploaded to the Python package registry, including the scripts that are used to run the experiments here. You can download it directly with {\tt pip install DeepGProp}. It has been released under the GNU GPL v3, and between releases it's also available from its GitHub repository \url{https://github.com/lulivi/dgp-lib}.

Since we have implemented {\sf EvoMLP} using the Python programming language. Tests with pypy, an accelerated, just-in-time version of the regular interpreter were performed, but although it is supposed to accelerate most numerical workloads, it actually was almost twice as slow as the one using the regular interpreter.
% Do we want to talk about technical implementation details as suggested in
% #10? Explaining all the code and that stuff - Luis
% In the abstract the bringing to modern languages and tools is highlighted
% Here could be a good place to present and justify these implementation details - M
% But what more detail could we present? JJ

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sec:res}

Initial results comparing EvoMLP with G-Prop under different circumstances were already presented in \cite{deep-g-prop}. In general, allowing the evolution to optimize the number of layers allowed to obtain significantly better results than the original results published by G-Prop. Since then, however, new datasets have been published and we will need to establish a new baseline, as well as characterize the size of problems that are suitable for optimization with this package.

In this paper we will first try to show the performance of the algorithm for this kind of problem, as well as create a baseline for comparison as we evolve the framework and the underlying algorithm. For that purpose, we have chosen {\sf spambase} as the dataset for all experiments. Taken from the UCI \cite{uci} database, it is a highly unbalanced dataset, with a good amount of features, and has been used extensively for benchmarking frameworks and algorithms. In papers such as the one by Duriqi et al. \cite{duriqi2016comparative}, which tests the algorithms implemented in WEKA, the accuracy reported is around 90\%, although it's not clear how the data set was split and if it is measured with a part of the dataset unseen before in any way. In order to measure generalization, in our experiments we have followed the PROBEN1 convention \cite{Prechelt94c} which is equivalent to a three-fold cross validation, splitting the dataset in three parts and sequentially using two of them for training and validation, finally testing them with the third part. By testing the models with an unseen part of the dataset, we guarantee that we are measuring its generalization capability, and not overfitting the model to the specific data it has been trained with.
%
\begin{table*}
\centering
<<spambase.table,echo=FALSE, results="asis">>=
spambase1.base <- read.csv("data/spambase1-p64.csv")

spambase <- data.frame( partition = "spambase1",
                       best.validation = min(spambase1.base$Validation),
                       median.validation = median(spambase1.base$Validation),
                       average.validation = mean(spambase1.base$Validation),
                       best.test = min(spambase1.base$Test),
                       median.test = median(spambase1.base$Test),
                       average.test = mean(spambase1.base$Test),
                       sd.test = sd(spambase1.base$Test))

comparison <- data.frame( partition = "spambase1 Theano",
                       best.validation = min(spambase1.base$Validation),
                       median.validation = median(spambase1.base$Validation),
                       best.test = min(spambase1.base$Test),
                       median.test = median(spambase1.base$Test),
                       sd.test = sd(spambase1.base$Test))

spambase2.base <- read.csv("data/spambase2-p64.csv")

spambase <- rbind( spambase,
                  data.frame( partition = "spambase2",
                       best.validation = min(spambase2.base$Validation),
                       median.validation = median(spambase2.base$Validation),
                       average.validation = mean(spambase2.base$Validation),
                       best.test = min(spambase2.base$Test),
                       median.test = median(spambase2.base$Test),
                       average.test = mean(spambase2.base$Test),
                       sd.test = sd(spambase2.base$Test) ) )

spambase3.base <- read.csv("data/spambase3-p64.csv")

spambase <- rbind( spambase,
                  data.frame( partition = "spambase3",
                       best.validation = min(spambase3.base$Validation),
                       median.validation = median(spambase3.base$Validation),
                       average.validation = mean(spambase3.base$Validation),
                       best.test = min(spambase3.base$Test),
                       median.test = median(spambase3.base$Test),
                       average.test = mean(spambase3.base$Test),
                       sd.test = sd(spambase1.base$Test) ) )

options(knitr.table.format = "latex")
knitr::kable(spambase, col.names=c("Partition","Validation,best","Validation, median","Validation,mean", "Test, best", "Test, median", "Test, average", "Test, SD"))
@
\vspace*{1mm}
\caption{Baseline error percentage results for {\sf spambase} per partition, with validation (used by the algorith) and test (unseen data, used for generalization) results.}
\label{tab:spambase:base}
\end{table*}

We are going to use the arguments shown in Table \autoref{tab:params}. Just the parameters indicated with * differ from the default values.
%
\begin{table}
\centering
\begin{tabular}{|l|r|}
\hline
Parameter & Value \\
Population & 64* \\
Max generations & 32* \\
Number of hidden layers & 1-3 \\
Crossover probability & 0.5 \\
Neuron bias mutation probability & 0.2 \\
Weight mutation probability & 0.75 \\
Probability to add/delete neurons & 0.3 \\
Probability to add/delete layers & 0.3 \\
Number of neurons per layer & 2-20 \\
\hline
\end{tabular}
\vspace*{1mm}
\caption{Parameters used in these experiments. An asterisk indicates non-default parameters.}
\label{tab:params}
\end{table}
%

In general, these values have been found by exploration; a further sensitivity study is probably needed, but meanwhile we try to use a reasonable amount of exploitation with exploration that is more intense for weights, and less intense for the other possibilities of change. This version will use Python 3.8.6, as well as Theano 2.3.1 with a Theano backend; Theano is a good choice, and it was proved to be slightly faster than other backends, like Tensorflow. Results are shown in \autoref{tab:spambase:base}. That table represents the average over {\em all} individuals in the last generation. We should take into account the uncertainty in the evaluation of the fitness via training and testing, so the concept of {\em best} individual in the last generation includes the same amount of uncertainty. 


%
\begin{table*}
\centering
<<comparison.table,echo=FALSE, results="asis">>=
spambase1.tf <- read.csv("data/cec-2021-spambase-1.csv")
spambase2.tf <- read.csv("data/cec-2021-spambase-2.csv")
spambase3.tf <- read.csv("data/cec-2021-spambase-3.csv")

comparison <- rbind(comparison,
                    data.frame( partition = "spambase1 TF",
                       best.validation = min(spambase1.tf$Validation),
                       median.validation = median(spambase1.tf$Validation),
                       best.test = min(spambase1.tf$Test),
                       median.test = median(spambase1.tf$Test),
                       sd.test = sd(spambase1.tf$Test)))
                    
comparison <- rbind( comparison,
                    data.frame( partition = "spambase2 Theano",
                       best.validation = min(spambase2.base$Validation),
                       median.validation = median(spambase2.base$Validation),
                       best.test = min(spambase2.base$Test),
                       median.test = median(spambase2.base$Test),
                       sd.test = sd(spambase2.base$Test) ) )                    

comparison <- rbind( comparison,
                      data.frame( partition = "spambase2 TF",
                       best.validation = min(spambase2.tf$Validation),
                       median.validation = median(spambase2.tf$Validation),
                       best.test = min(spambase2.tf$Test),
                       median.test = median(spambase2.tf$Test),
                       sd.test = sd(spambase2.tf$Test) ) )

comparison <- rbind( comparison,
                  data.frame( partition = "spambase3 Theano",
                       best.validation = min(spambase3.base$Validation),
                       median.validation = median(spambase3.base$Validation),
                       best.test = min(spambase3.base$Test),
                       median.test = median(spambase3.base$Test),
                       sd.test = sd(spambase1.base$Test) ) )

comparison <- rbind( comparison,
                  data.frame( partition = "spambase3 TF",
                       best.validation = min(spambase3.tf$Validation),
                       median.validation = median(spambase3.tf$Validation),
                       best.test = min(spambase3.tf$Test),
                       median.test = median(spambase3.tf$Test),
                       sd.test = sd(spambase1.tf$Test) ) )
options(knitr.table.format = "latex")
knitr::kable(comparison, col.names=c("Partition","Validation,best","Validation, median", "Test, best", "Test, median", "Test, SD"))
@
\vspace*{1mm}
\caption{Comparison of baseline (Theano-based) and the last version, based on Tensorflow, error percentage results for {\sf spambase} per partition, with validation (used by the algorithm) and test (unseen data, used for generalization) results.}
\label{tab:spambase:comparison}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and future work}
\label{sec:conclus}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}

This paper has been supported in part by project DeepBio (TIN2017-85727-C4-2-P)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\bibliography{geneura,deep-g-prop,gprop,gpropnpl}

\end{document}
