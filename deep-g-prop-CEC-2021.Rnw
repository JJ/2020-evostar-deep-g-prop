\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{hyperref}

<<setup, cache=FALSE,echo=FALSE,message=FALSE,warning=FALSE>>=
require(ggplot2)
require(ggthemes)
@

\begin{document}
%
\title{EvoMLP: a framework for evolving multilayer perceptrons}
%
\author{\IEEEauthorblockN{Luis Liñán-Villafranca}
  \IEEEauthorblockA{\textit{RTI} \\
    {\tt luis@rti.com}}
  \and
  \IEEEauthorblockN{Mario Garc\'ia-Valdez}
  \IEEEauthorblockA{\textit{Instituto Tecnol\'ogico de Tijuana} \\
    {\tt mario@tectijuana.edu.mx}}
  \and
  \IEEEauthorblockA{J.J. Merelo, Pedro Castillo-Valdivieso}
  \IEEEauthorblockN{Computer Architecture and Technology\\
    University of Granada, Spain\\
    {\tt {jmerelo|pacv}@ugr.es}}
}
%
\maketitle              % typeset the header of the contribution

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Designing neural networks for classification or regresion can be
considered a search problem, and, as such, can be approached using
different optimization procedures. There are also several design
challenges. The first and more important is to constrain the search
space in such a way that proper solutions can be found in a reasonable
amount of time; the second is to take into account that, depending on
how the optimization procedure is formulated, the fitness score used
for it can have a certain degree of uncertainty. This means that
creating a framework for evolving neural networks for classification
implies taking a series of decisions that range from the purely
technical to the algorithmical at different levels: neural or the
optimization framework chosen. This will be the focus of this paper,
where we will introduce DeepGProp, a framework for genetic
optimization of multilayer perceptrons, that is able to explore the
space of neural nets with different layers and layer size and find
good solutions in a reasonable amount of time. The different design
decisions taken and how they affect the output will be presented and
compared with other state of the art frameworks.

\end{abstract}

\begin{IEEEkeywords}
  Neuroevolution, Python, Backpropagation, multilayer perceptrons,
  software frameworks, MLPs.
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Applying artificial neural networks (ANN) to the solution of
classification problems requires establishing their structure in
layers and connections between them, the initial parameters (such as
initial weights) and a set of learning constants. 
Using an optimization algorithm to solve at least part of this design problem
is an usual approach to this challenge, and evolutionary neural networks \cite{GOMESPEREIRADELACERDA2021100777,Martinez2021LightsAS} have been proved to be an efficient way of searching for the
architecture, weights, and other ANN parameters (such as learning
constants). This has been proved
repeatedly for {\em shallow} architectures (with a single hidden
layer) since late in the previous century
\cite{yao1993evolutionary,CastilloNPL,stanley2002evolving}. However, in most cases, design
of the structure and connections between different layers, as
indicated by \cite{miikkulainen2019evolving}, is still mostly done by
hand and left off the evolutionary process. This is mainly due to the above mentioned fact that including this makes search
spaces huge, so using rules of thumb (such as the ones mentioned
in \cite{qolomany2017parameters}, or using certain formulas to set the
number of layers) to decide on those parameters,
are a way of fixing part of the search space, letting the neural net training algorithm to figure out the weights themselves, whose
optimization, in these cases, is able to cover big spans of the search
space. Some of these algorithms stop short of setting weights, and leave that to an actual neural net training algorithm. The fitness of the neural net is measured {\em after} training, thus including training time within the fitness evaluation time. Adding long evaluation time to the size of the search space makes, sometimes, the mixture of evolutionary algorithms with neural networks something that can be approached only with big evaluation budgets.

That high budget needed needs to be managed, and this is done in several ways, usually. One of them is to fix a part of the architecture, by choosing a
multi-layer perceptron (MLPs) using some well-proven training algorithm. After all, MLPs with a single hidden layer are universal approximators, and have been used extensively for classification and regression problems. Once that part of the search space is fixed, the initial
weights as well as biases will evolve using an evolutionary algorithm (EA), and some MLP training algorithm such as backpropagation will be used to find the final MLP that's tested and eventually produced as the solution to a problem. This algorithm was proposed in the early years of the century, and called \emph{G-Prop} (as in \emph{Genetic Back-Propagation}), originally presented in \cite{castilloNC,CastilloNPL}. G-Prop evolves a population of data structures representing multi-layer perceptrons initial weights and biases, which are trained and tested to obtain the fitness. 
This method leverages the capabilities of two classes of algorithms: the
ability of EA to find a solution close to the global optimum, and the
ability of the back-propagation algorithm (BP) \cite{Rumelhart86} to tune a solution and
reach the nearest local minimum by means of local search from the
solution found by the EA.

This initial G-Prop algorithm was limited, first, by the fact that
most of it had to be programmed from scratch, since there were few (if
any) off-the-shelf software that could do this kind of task;
additionally, available computational capability limited the size of
the search space; this was further limited by the evolution of only single-layer perceptrons. This is why only small problems could be approached, and the evaluation budget used in them was, forcibly, limited.

However, the {\em memetic} combination of optimization at two levels, the weight level (handled by backpropagation) and the parametrized architecture level (handled by an evolutionary algorithm), is still conceptually new, since not many methods use them. Besides, the implementation, even being open source, is not maintained. This is why in this paper we propose, first, a reimplementation of the G-Prop concept, and second, an extension that solves some problems with this initial method: the inability to work with neural nets with several layers, and also the implicit dealing with the uncertainty in computing the fitness using a stochastic method like neural net training.

Thus, the aim of this paper is to present EvoMLP, a neuroevolution framework that evolves the initial weights, number of
hidden layers and hidden layer sizes of a MLP, based on a EA and
Stochastic Gradient Descent (SGD) \cite{bottou2012stochastic}. This
algorithm has been used for training deep neural nets and other kind
of machine learning algorithms
\cite{bottou2012stochastic,bottou2010large}; however, in this case
intermediate (or hidden) layers have the same structure,
which is the structure of a multi-layer perceptron, justifying our
denomination of {\sf EvoMLP} for this specific method.

In this paper what we will do is to first bring the implementation of
the G-Prop concept to current tools and languages, choosing the best
suited for the purpose and show the design decisions that were taken in order to do so; since we are using off the shelf software libraries which are in constant evolution, we will also show, since implementation matters \cite{DBLP:conf/iwann/MereloRACML11}, how this change of versions affects (or not) the performance of the algorithm. We will also attempt to characterize the size of the problems for which this type of algorithm can be considered useful.

The remainder of this paper is structured as follows:
section \ref{soa} makes a short overview of the methods to design ANN.
In section \ref{method} it is fully described the proposed model.
Section \ref{sec:res} describes the results obtained,
followed by a brief conclusion in Section \ref{sec:conclus}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{State of the Art}
\label{soa}

Searching the parameter space of neural networks looking for optimal combinations  has been a problem traditionally approached
in a limited number of ways, using {\em incremental algorithms}, which, in
general, will depart from a specific architecture, number of layers
and elements in every layer, and offer a series of heuristic
procedures to add/eliminate layers and/or hidden-layer neurons in
% I think at this time there are only input and output layers (and these must remain fixed) 
% and we have the option of adding and removing only hidden layers. 
% I propose to only say: to add/eliminate layers and the number of neurons in them 
% In more advanced networks there are other types like  recurrent, convolution etc. 
% but making the distinction is a bit confusing - M 
them. This way was, for a certain amount of time, the preferred % the preferred technique?
one for optimizing the size of the neural net along with its weights, according to classical reviews such % architecture of the nn ? - M
as the one by Alpaydin \cite{alpaydin1994gal}. However and simultaneously, a
series of evolutionary heuristics were starting to be applied for the
same purpose, as revealed by Balakrishnan and Honavar's review
\cite{balakrishnan95:EDNA}. They propose a taxonomy, that classifies evolutionary
design of the architecture of neural nets along three different axes
(plus one for application domain): genotype representation, network
topology, and a third axis that includes basically everything else,
``variables of evolution''.
This classification was much more comprehensive, and included many more degrees of freedom in evolution than simply adding or eliminating neurons. However, it still left some constraints and probably did not emphasize the parameters that are most likely to have the greater influence in the outcome.

This is why, form our point of view, a more precise classification of evolved neural architectures would include: \begin{itemize}
  \item Fixed or variable architecture. Irrespective of the kind of
    data structured used to represent it, what is relevant is if that
    representation allows for a variation of the architectural
    parameters, that is, adding or eliminating new nodes, layers, or
    in general, nodes and edges in a graph representation of the
    architecture.
  \item Articulation of the neural learning method. There is a whole
    range of possibilities, from letting the evolutionary process set
    all weights and other parameters, to using a learning process just
    to find the success rate, with additional possibilities like
    evaluating success {\em before applying learning} (to take
    advantage of the Baldwin effect \cite{castillo-2006}) to applying
    a few cycles of learning that will become part of the codified
    solution.
  \item How the lack of a "crisp" value for the fitness, is taken into account
    % I think some adjective is missing for the kind of determination
    % The lack of a "crisp"|"optimum"|"hard"|"definite"  determination? - M
    % solved - JJ
    during evolution. Either from the fact that an stochastic algorithm is
    used to train the neural net before evaluating fitness, or the
    fact that the data set (or how it's presented for training) might not be totally fixed but subject to
    randomness, the fitness of a neural net cannot be pinned down to a
    single, crisp, number. This randomness in the fitness can be taken
    into account during the selection phase
    \cite{DBLP:conf/ijcci/MereloLFGCCRMG15}, or not. This is related to 
    having models that can generalize better when classifying unseen data,
    reducing overfitting. 
    % Please check or extend the above sentence - M
    % There is also the problem of overfitting - M
    % It's OK - JJ
  \item Mono or multiobjective optimization. In general, the main issue is to get the highest accuracy possible. However, 
  several other factors can be taken into account; in unbalanced data sets, accuracy for every one of the categories must be taken into account separately; besides, it is almost impossible to optimize by generalization, so a proxy for it, size, is instead optimized. All these objectives can be aggregated into a single one (using weights), hierarchized or considered properly as different objectives to be optimized separately.
 % Single or multiobjective?
 % There is more research in this aspect, changing "size" by "entropy", "message length" etc., there
 % is much more future work to do, just in this aspect alone. - M 
 % In the area of decision tree classifiers there are many estimators of their size/complexity.

\end{itemize}

With these axes in mind, the first one, the variation of architecture, has been the one that has developed the most,  lately evolving towards (almost)
unconstrained evolution using Neuroevolution of Augmenting Topologies (NEAT), as shown in the recent review by
Miikkulainen et al. \cite{miikkulainen2019evolving}. Since this blows
up the search space (essentially evolving graphs), these methods need computational resources that are
really beyond the reach of most labs or individuals. This is the reason why most of the latest publications 
still use a fixed model for the network architecture, for instance SVM % support vector machines? these are not NN,
% or is it a kind of network I don't know of? - M
or MLPs, and even, in some cases, fixed number of layers and weights
\cite{ecer2020training}; however, recent papers like the one by
Tajmiri et al. \cite{TAJMIRI2020108997} already use a representation
that is able to accommodate a maximum number of layers, as well as the
number of neurons in every layer. This encoding of
architecture does not have any place for initial weights, which in the
case of G-Prop, were proved to be essential for a good performance,
including the emergence of a Baldwin effect \cite{castillo-2006}
during evolution.
On the other hand, lately multi-objective algorithms are being explored. Senhaji et al. \cite{SENHAJI20201} optimizes performance at the same time that minimizes size using the NSGA-II algorithm. This is a generally good strategy, once again involving, as is usual in multiobjective algorithms, a boost in the size of the search space requiring big evaluation budgets.

In this paper we will extend the principles of variation of a MLP
architecture through the use of genetic operators as well as
codification of initial weights to a modern, standard based
implementation, that will also be released as free software. We show
how this new {\sf EvoMLP} method works next. This means it will again use the architecture axis of evolution, although constraining it to a particular type of neural nets so that, by constraining search, we shrink the search space and make finding good architectures easier.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Method}
\label{method}

%The general the algorithm will proceed like the \autoref{alg:ga} shows.

% \begin{algorithm}
%   \caption{Genetic Algorithm loop}\label{alg:ga}
%   \begin{algorithmic}[1]
%     \State \textit{evaluate initial population}
%     \While{\textit{max fit is less than ideal and there are generations left}}
%     \If{\textit{max fit hasn't improved in some generations}}
%     \State \textit{Stop algorithm}
%     \EndIf
%     \State \textit{select the offspring from the best population individuals}
%     \State \textit{apply crossover to the offspring}
%     \State \textit{apply mutations to the offspring}
%     \State \textit{replace the worst population individuals with the modified offspring}
%     \EndWhile
%   \end{algorithmic}
% \end{algorithm}

In this work we will use a canonical evolutionary algorithm, as implemented by DEAP. The key of the adaptation of the algorithm for the neuroevolution task we are doing here is in the different parts used, that extend and enhance those used by the
original G-Prop. % Briefly comment on these enhancements, where are they? - M 
                 % (Update) This is done in some places but maybe you should do another pass to see if there are more 
                 % enhancements - M 

\begin{itemize}

  \item An individual is coded as a list of \emph{Layers}. Each layer is
    composed by a matrix of weights and a list of bias (both Numpy
    \cite{py-numpy} ndarrays) joined together with it's configuration (a
    Python dictionary) in a Python class. One column of the weight matrix
    represents all the connections from the previous layer to the nth neuron.
    All the weights and bias are randomly initialized with a uniform
    distribution between -1.0 and 1.0. A random proportion of all this genes
    and biases are muted for all the individuals each generation.

    From this configuration, when the moment of evaluation comes, we create a
    \emph{Sequential} \cite{keras-sequential}  Keras \cite{keras-nn} model and
    append each \emph{Layer} as a \emph{Dense} \cite{keras-dense} Keras layer.

  \item There are three stop conditions that will finish the execution of the
    genetic algorithm: if we reach the maximum number of generations, if after
    10 generations there was no improvement in the best solution and if we find
    the ideal solution ($0\%$ error, 1 on the hidden-score and 1 on the F2
    score).
    % Justify why after 10 generations, or even better make that a parameter, set in this
    % experiments to 10 - M

  \item For each generation we clone the best half individuals of the current
    population and perform all the evolutionary operations to them. Each
    operation has it own probability to occur chosen when running EvoMLP
    CLI. For the weights/bias mutation, a percentage of each (selected too with
    the CLI parameters) are muted randomly. After that, the worst half is
    replaced by this muted group.

  \item The {\em crossover} function will swap the ``neurons'' together with
    their connections. Main difference with respect to original G-Prop is that,
    in this case, there can be several hidden layers, so the connections might
    be either to output or to another layers. The crossover only occurs when
    the two models selected for it have the same structure (same number of
    layers and neurons per layers).
    
    % For future work: the crossover could select structure of one of the parents or mix-them. - M
    % Question: only the initial weights are evolved? or the search is mixed with BP? 
    % i.e., adjust after a few epochs or update the weight matrix after each fitness eval - M
    % Just the initial weights - JJ
    
  \item There are several kinds of {\em mutation} operators: (initial) weights,
    biases, as well as changing the number of neurons and/or
    layers. For the neuron mutator, one neuron will be added last in the
    randomly selected layer, and it will be eliminated in the same way. Since
    we use fully connected neural networks, it does not really matter where the
    neuron is added or eliminated. The layer add/eliminate mutator is the
    main difference with respect to G-Prop, which used only one
    layer. In this case, only the last hidden layer can be eliminated. When a
    layer is eliminated, the weights from the previous layer to the eliminated
    one are kept, adding randomly generated weights and biases if the output
    layer (newly connected with this previous layer) have more ``neurons'' than
    the recently deleted layer. When adding a new hidden layer, the connections
    between the last hidden layer and the output layer are recycled, and the
    connections between the newly created layer and the output layer are
    calculated randomly (running similar fix as when we delete the layer).

\end{itemize}

We need the selection process, via the fitness function, to exert parsimony pressure, which is why
we take into account not only the accuracy score reached after
training, but also the overall size of the network using this score: 
\begin{equation}
f_{s} = (\sum_{i=0}^{nhl} nu_{i}) \times nhl
\end{equation}
where $f_{s}$, is the hidden-layer, or size, score, $nhl$ is the number of hidden layers and $nu_{i}$ the number of neurons for the layer $i$). This score is minimized, meaning that the less neurons, the better, but also that less hidden layers are preferred over more hidden layers. This $f_s$, however, receives a very low weight equivalent to 1\% of the accuracy, meaning that it will be only determinant, in practice, if there's a very small difference in accuracy when comparing two individuals of if accuracy is virtually the same.


Finally, the F2 score metric (explained in the
\autoref{sec:res}) is also considered. At the end, the fitness value is a compound of
three elements: accuracy, hidden-score and F2 score. As the DEAP framework
\cite{deap-ga} was used as a base to create the genetic algorithm, we also
took advantage of its fitness class \cite{deap-fitness} to rank the individuals
of a population. It performs a lexicographical comparison of the three values,
so we chose the order previously mentioned to approach as much as possible to
the fitness function used by G-Prop.
% Question for @jj related with #28. Do we need the whole expression? I don't
% really know how to create an expression for the fitness score since it is a
% tuple - Luis This is important because is related to many principles of ML
% You just return a single value as a tuple, like this: return fitness,  (notice the comma at the end) - M
%
%
% \begin{figure}
%     \centering
%     \caption{
%         {\sf EvoMLP} NN model score equation.
%     }
%     \label{eq:hidden-score}
%     \begin{equation}
%         
%     \end{equation}
% \end{figure}
%
So in summary, the score will be the
global number of neurons multiplied by the number of hidden layers.
%
The whole code for this has been released under a free license in
GitHub. % Add URL later.

{\sf EvoMLP} has been implemented using the Python programming language. Tests with pypy, an accelerated, just-in-time version of the regular interpreter were tested, but although it is supposed to accelerate most numerical workloads, it actually was almost twice as slow as the one using the regular interpreter. There are
many reasons to use this language, but two of the most significant reasons are
the extensive documentation, use and community it has, and the vast amount of
ML ready frameworks that are developed for it. Keras and DEAP are two impressive
frameworks currently in the state-of-art on their respective area.
% Do we want to talk about technical implementation details as suggested in
% #10? Explaining all the code and that stuff - Luis
% In the abstract the bringing to modern languages and tools is highlighted
% Here could be a good place to present and justify these implementation details - M

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sec:res}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and future work}
\label{sec:conclus}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}

This paper has been supported in part by project DeepBio (TIN2017-85727-C4-2-P)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\bibliography{geneura,deep-g-prop,gprop,gpropnpl}

\end{document}
