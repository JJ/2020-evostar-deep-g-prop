\documentclass[runningheads]{llncs}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{hyperref}

<<setup, cache=FALSE,echo=FALSE,message=FALSE,warning=FALSE>>=
require(ggplot2)
require(ggthemes)
@

\begin{document}
%
\title{EvoMLP: a framework for evolving multilayer perceptrons}
%
\author{Luis Liñán-Villafranca \inst{1}
  \and
  Mario Garc\'ia-Valdez \inst{2}
  \and
  J.J. Merelo, Pedro Castillo-Valdivieso \inst{3}
}

\institute{RTI \email{luis@rti.com}
\and
Instituto Tecnol\'ogico de Tijuana \email{mario@tectijuana.edu.mx}
\and
Universidad de Granada \email{ {jmerelo|pacv}@ugr.es}}
%
\maketitle              % typeset the header of the contribution

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Designing neural networks for classification or regression can be
considered a search problem, and, as such, can be approached using
different optimization procedures. There are also several design
challenges. The first and more important is to constrain the search
space in such a way that proper solutions can be found in a reasonable
amount of time; the second is to take into account that, depending on
how the optimization procedure is formulated, the fitness score used
for it can have a certain degree of uncertainty. This means that
creating a framework for evolving neural networks for classification
implies taking a series of decisions that range from the purely
technical to the algorithmic at different levels: neural or the
optimization framework chosen. This will be the focus of this paper,
where we will introduce DeepGProp, a framework for genetic
optimization of multilayer perceptrons, that is able to explore the
space of neural nets with different layers and layer size and find
good solutions in a reasonable amount of time. The different design
decisions taken and how they affect the output will be presented and
compared with other state of the art frameworks.

\end{abstract}

\begin{keywords}
  Neuroevolution, Python, Backpropagation, multilayer perceptrons,
  software frameworks, MLPs.
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Applying artificial neural networks (ANN) to the solution of
classification problems requires establishing their structure in
layers and connections between them, the initial parameters (such as
initial weights) and a set of learning constants. 
Using an optimization algorithm to solve at least part of this design problem
is an usual approach to this challenge, and evolutionary neural networks \cite{GOMESPEREIRADELACERDA2021100777,Martinez2021LightsAS} have been proved to be an efficient way of searching for the
architecture, weights, and other ANN parameters (such as learning
constants). This has been demonstrated
repeatedly for {\em shallow} architectures (with a single hidden
layer) since late in the previous century
\cite{yao1993evolutionary,CastilloNPL,stanley2002evolving}. However, in most cases, design
of the structure and connections between different layers, as
indicated by \cite{miikkulainen2019evolving}, is still mostly done by
hand and left off the evolutionary process. This is mainly due to the above mentioned fact that including this makes search
spaces huge, so using rules of thumb (such as the ones detailed
in \cite{qolomany2017parameters}, or using certain formulas to set the
number of layers) to decide on those parameters,
are a way of fixing part of the search space, letting the neural net training algorithm to figure out the weights themselves, whose
optimization, in these cases, is able to cover big spans of the search
space. Some of these algorithms stop short of setting weights, and leave that to an actual neural net training algorithm. The fitness of the neural net is measured {\em after} training, thus including training time within the fitness evaluation time. Adding long evaluation time to the size of the search space makes, sometimes, the mixture of evolutionary algorithms with neural networks something that can be approached only with big evaluation budgets.

That high budget required needs to be managed, and this is done in several ways. Usually one of them is to fix a part of the architecture, by choosing a
multi-layer perceptron (MLPs) using some well-proven training algorithm. After all, MLPs with a single hidden layer are universal approximators, and have been used extensively for classification and regression problems. Once that part of the search space is covered, the initial
weights as well as biases will evolve using an evolutionary algorithm (EA), and some MLP training algorithm such as backpropagation will be used to find the final MLP that's tested and eventually produced as the solution to a problem. This algorithm was proposed in the early years of the century, and called \emph{G-Prop} (as in \emph{Genetic Back-Propagation}), originally presented in \cite{castilloNC,CastilloNPL}. G-Prop evolves a population of data structures representing multi-layer perceptrons initial weights and biases, which are trained and tested to obtain the fitness. 
This method leverages the capabilities of two classes of algorithms: the
ability of EA to find a solution close to the global optimum, and the
ability of the back-propagation algorithm (BP) \cite{Rumelhart86} to tune a solution and
reach the nearest local minimum by means of local search from the
solution found by the EA.

This initial G-Prop algorithm was limited, first, by the fact that
most of it had to be programmed from scratch, since there were few (if
any) off-the-shelf software that could do this kind of task;
additionally, available computational capability limited the size of
the search space; this was further limited by the evolution of only single-layer perceptrons. This is why only small problems could be approached, and the evaluation budget used in them was, forcibly, limited.

However, the {\em memetic} combination of optimization at two levels, the weight level (handled by backpropagation) and the parametrized architecture level (handled by an evolutionary algorithm), is still conceptually new, since not many methods use them. Besides, the implementation, even being open source, is not maintained. This is why in this paper we propose, first, a reimplementation of the G-Prop concept, and second, an extension that solves some problems with this initial method: the inability to work with neural nets with several layers, and also the implicit dealing with the uncertainty in computing the fitness using a stochastic method like neural net training.

Thus, the aim of this paper is to present EvoMLP, a neuroevolution framework that evolves the initial weights, number of
hidden layers and hidden layer sizes of a MLP, based on a EA and
Stochastic Gradient Descent (SGD) \cite{bottou2012stochastic}. This
algorithm has been used for training deep neural nets and other kind
of machine learning algorithms
\cite{bottou2012stochastic,bottou2010large}; however, in this case
intermediate (or hidden) layers have the same structure,
which is the structure of a multi-layer perceptron, justifying our
denomination of {\sf EvoMLP} for this specific method.

In this paper what we will do is to first bring the implementation of
the G-Prop concept to current tools and languages, choosing the best
suited for the purpose and show the design decisions that were taken in order to do so; since we are using off the shelf software libraries which are in constant evolution, we will also demonstrate, since implementation matters \cite{DBLP:conf/iwann/MereloRACML11}, how this change of versions affects (or not) the performance of the algorithm. Apart from that, we will also attempt to characterize the size of problems for which this type of algorithm can be considered useful.

The remainder of this paper is structured as follows:
section \ref{soa} makes a short overview of the methods to design ANN.
In section \ref{method} it is fully described the proposed model.
Section \ref{sec:res} describes the results obtained,
followed by a brief conclusion in Section \ref{sec:conclus}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{State of the Art}
\label{soa}

Searching the parameter space of neural networks looking for optimal combinations  has been a problem traditionally approached
in a limited number of ways, using {\em incremental algorithms}, which, in
general, will depart from a specific architecture, number of layers
and elements in every layer, and offer a series of heuristic
procedures to add/eliminate hidden-layers and/or neurons from them.
% I think at this time there are only input and output layers (and these must remain fixed) 
% and we have the option of adding and removing only hidden layers. 
% I propose to only say: to add/eliminate layers and the number of neurons in them 
% In more advanced networks there are other types like  recurrent, convolution etc. 
% but making the distinction is a bit confusing - M 
% Can you please ellaborate on this? Or simply rewrite it, no problem - JJ
% I think this is what Mario was refering - L
This way was, for a certain amount of time, the preferred technique
for optimizing the architecture of the neural net along with its weights, according to classical reviews such % architecture of the nn ? - M
% don't get it, sorry - JJ
as the one by Alpaydin \cite{alpaydin1994gal}. However and simultaneously, a
series of evolutionary heuristics were starting to be applied for the
same purpose, as revealed by Balakrishnan and Honavar's review
\cite{balakrishnan95:EDNA}. They propose a taxonomy, that classifies evolutionary
design of the architecture of neural nets along three different axes
(plus one for application domain): genotype representation, network
topology, and a third axis that includes basically everything else:
``variables of evolution''.
This classification was much more comprehensive, and included many more degrees of freedom in evolution than simply adding or eliminating neurons. However, it still left some constraints and probably did not emphasize the parameters that are most likely to have the greater influence in the outcome.

This is why, from our point of view, a more precise classification of evolved neural architectures would include: \begin{itemize}
  \item Fixed or variable architecture. Regardless of the kind of
    data structure used to represent it, the relevant matter is if that
    representation allows a variation of the architectural
    parameters, that is, adding or eliminating new nodes, layers, or
    in general, nodes and edges in a graph representation of the
    architecture.
  \item Articulation of the neural learning method. There is a whole
    range of possibilities, from letting the evolutionary process set
    all weights and other parameters, to using a learning process just
    to find the success rate, with additional possibilities like
    evaluating success {\em before applying learning} (to take
    advantage of the Baldwin effect \cite{castillo-2006}), to running
    a few cycles of learning that will become part of the codified
    solution.
  \item How the lack of a "crisp" value for the fitness, is taken into account
    during evolution. Either from the fact that an stochastic algorithm is
    used to train the neural net before evaluating fitness, or the
    fact that the data set (or how it's presented for training) might not be totally fixed but subject to
    randomness, the fitness of a neural net cannot be pinned down to a
    single, crisp, number. This randomness in the fitness can be taken
    into account during the selection phase
    \cite{DBLP:conf/ijcci/MereloLFGCCRMG15}, or not. This is related to 
    having models that can generalize better when classifying unseen data,
    reducing overfitting. 
    % Please check or extend the above sentence - M
    % There is also the problem of overfitting - M
    % It's OK - JJ
  \item Single or multiobjective optimization. In general, the main issue is to get the highest accuracy possible. However, 
  several other factors can be taken into account; in unbalanced data sets, accuracy for every one of the categories must be taken into account separately; besides, it is almost impossible to optimize by generalization, so a proxy for it, size, is instead optimized. All these objectives can be aggregated into a single one (using weights), hierarchized or considered properly as different objectives to be optimized separately.
 % Single or multiobjective?
 % There is more research in this aspect, changing "size" by "entropy", "message length" etc., there
 % is much more future work to do, just in this aspect alone. - M 
 % In the area of decision tree classifiers there are many estimators of their size/complexity.

\end{itemize}

With these axes in mind, the first one, the variation of architecture, has been the one that has developed the most,  lately evolving towards (almost)
unconstrained evolution using Neuroevolution of Augmenting Topologies (NEAT), as shown in the recent review by
Miikkulainen et al. \cite{miikkulainen2019evolving}. Since this blows
up the search space (essentially evolving graphs), these methods need computational resources that are
really beyond the reach of most labs or individuals. This is the reason why most of the latest publications 
still use a fixed model for the network architecture, for instance SVM % support vector machines? these are not NN,
% or is it a kind of network I don't know of? - M
or MLPs, and even, in some cases, fixed number of layers and weights
\cite{ecer2020training}; however, recent papers like the one by
Tajmiri et al. \cite{TAJMIRI2020108997} already use a representation
that is able to accommodate a maximum number of layers, as well as the
number of neurons in every layer. This encoding of
architecture does not have any place for initial weights, which in the
case of G-Prop, were proved to be essential for a good performance,
including the emergence of a Baldwin effect \cite{castillo-2006}
during evolution.
On the other hand, lately multi-objective algorithms are being explored. Senhaji et al. \cite{SENHAJI20201} optimizes performance at the same time that minimizes size using the NSGA-II algorithm. This is a generally good strategy, once again involving, as is usual in multiobjective algorithms, a boost in the size of the search space requiring big evaluation budgets.

More generally, network architecture search algorithms are also model search algorithms \cite{mazzawi21}, a framework for which was recently released by Google. These methods are generally appropriate if you have unlimited evaluation budget or if problems are complex enough that only this kind of algorithms are able to find the correct model.

However, there is a huge amount of moderately-sized problems (with their corresponding datasets), where constrained search can be successful.
In this paper we will extend the principles of variation of a MLP
architecture through the use of genetic operators as well as
codification of initial weights to a modern, standard based
implementation, that will also be released as free software. We show
how this new {\sf EvoMLP} method works next. This means it will again use the architecture axis of evolution, although constraining it to a particular type of neural nets so that, by constraining search, we shrink the search space and make finding good architectures easier. Even so, the size of the datasets (and thus problems) that can be approached must be characterized, which we will do in the next sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Method}
\label{method}

There are two parts in this EvoMLP framework, the "neural" part and the "evolutionary" part. Our objective from the beginning was to use off-the-shelf frameworks as much as possible.

The first line of choice is the programming language. {\sf G-Prop} was written in C++ mainly because the underlying library used, EO \cite{EO:FEA2000}, used that language. In this case, we went in the opposite direction. Most popular high-performance machine learning libraries, like TensorFlow, are written in Python. Once we settled for TensorFlow, working with Python was the obvious choice.

There are indeed several evolutionary frameworks written in Python; EvoloPy, for instance \cite{DBLP:conf/ijcci/FarisAMCM16}, includes many hyperheuristics, including EAs. However, DEAP \cite{deap-ga} has emerged as a standard for this language, covering all that is needed to evolver multilayer perceptrons, so this is what we chose. This will definitely have some impact in performance \cite{kim2019software}, however for us researcher time is more important than computer time, and we can always use that time saved to improve the algorithm or the implementation.

TensorFlow is a low-level framework based on representing machine learning models as graphs; we will need a higher-level library to work with multilayer perceptrons. Once again, unlike G-Prop, we will choose the off-the-shelf Keras package \cite{keras-nn}, a framework for building multi-layer deep learning models In the version used in the initial stages of this research, Keras allowed multiple backends, meaning different frameworks for the effective representation of the neural net. That has changed lately, as we will later see. Once again, Keras is characterized for offering a simple API as opposed to a flexible and low-level interface, which is characteristic of TensorFlow or others like PyTorch; this has an impact on performance, but it helps ease the burden of the designers. The fact that it's maintained and updated often also ensures that the software created with it is kept up-to-date. This is important for a software framework like the one we're presenting here.

We need to make a series of decisions on the data structures and functions used in Keras; these are presented below. An individual is coded as a list of \emph{Layers}. Each layer is
    composed by a matrix of weights and a list of bias (both Numpy
    \cite{py-numpy} ndarrays) joined together with it's configuration (a
    Python dictionary) in a Python class. One column of the weight matrix
    represents all the connections from the previous layer to the nth neuron.
    All the weights and bias are randomly initialized with a uniform
    distribution between -1.0 and 1.0. A random proportion of all this genes
    and biases are muted for all the individuals each generation.

    From this configuration, when the moment of evaluation comes, we create a
    \emph{Sequential} \cite{keras-sequential}  Keras  model and
    append each \emph{Layer} as a \emph{Dense} \cite{keras-dense} Keras layer.

There are three stop conditions that will finish the execution of the
    genetic algorithm: if we reach the maximum number of generations, if after
    10 generations there was no improvement in the best solution and if we find
    the ideal solution ($0\%$ error, a single neuron left and 1 on the F2
    score). We used these 10 generations as an indication that evolution was stuck and could proceed no further, thus stopping being a better option.

For each generation we clone the best half individuals of the current
    population and perform all the evolutionary operations to them. Each
    operation has it own probability to occur chosen when running EvoMLP
    CLI. For the weights/bias mutation, a percentage of each (selected too with
    the CLI parameters) are muted randomly. After that, the worst half is
    replaced by this muted group.

The {\em crossover} function will swap the ``neurons'' together with
    their connections. Main difference with respect to original G-Prop is that,
    in this case, there can be several hidden layers, so the connections might
    be either to output or to another layers. The crossover only occurs when
    the two models selected for it have the same structure (same number of
    layers and neurons per layers).
    
    % For future work: the crossover could select structure of one of the parents or mix-them. - M
    % I don't know what you mean here, maybe you could directly add it to the future work section - JJ
    
There are several kinds of {\em mutation} operators: (initial) weights,
    biases, as well as changing the number of neurons and/or
    layers. For the neuron mutator, one neuron will be added last in the
    randomly selected layer, and it will be eliminated in the same way. Since
    we use fully connected neural networks, it does not really matter where the
    neuron is added or eliminated. The layer add/eliminate mutator is the
    main difference with respect to G-Prop, which used only one
    layer. In this case, only the last hidden layer can be eliminated. When a
    layer is eliminated, the weights from the previous layer to the eliminated
    one are kept, adding randomly generated weights and biases if the output
    layer (newly connected with this previous layer) have more ``neurons'' than
    the recently deleted layer. When adding a new hidden layer, the connections
    between the last hidden layer and the output layer are recycled, and the
    connections between the newly created layer and the output layer are
    calculated randomly (running similar fix as when we delete the layer).

We need the selection process, via the fitness function, to exert parsimony pressure, which is why
we take into account not only the accuracy score reached after
training, but also the overall size of the network using this score: 
\begin{equation}
f_{s} = (\sum_{i=0}^{nhl} nu_{i}) \times nhl
\end{equation}
where $f_{s}$, is the hidden-layer, or size, score, $nhl$ is the number of hidden layers and $nu_{i}$ the number of neurons for the layer $i$). This score is minimized, meaning that the less neurons, the better, but also that less hidden layers are preferred over more hidden layers. This $f_s$, however, receives a very low weight equivalent to 1\% of the accuracy, meaning that it will be only determinant, in practice, if there's a very small difference in accuracy when comparing two individuals of if accuracy is virtually the same.

Finally, the F2 score metric (explained in \autoref{sec:res}) is also considered. At the end, the fitness value is a compound of
three elements: accuracy, hidden-score and F2 score. As the DEAP framework
\cite{deap-ga} was used as a base to create the genetic algorithm, we also
took advantage of its fitness class \cite{deap-fitness} to rank the individuals
of a population. It performs a lexicographical comparison of the three values,
so we chose the order previously mentioned to approach as much as possible to
the fitness function used by G-Prop.

We handle uncertainty by {\em resampling}, that is, re-evaluating all individuals every generation. Every training-testing set will yield a different fitness, which usually have a skewed distribution \cite{DBLP:conf/ijcci/MereloLFGCCRMG15}. However, by repeatedly evaluating them individuals that have a statistically better fitness that the rest will {\em survive} to the next generation. This method, though simple, is generally adequate and tends to select individuals whose fitness distribution has a lower standard deviation and are less skewed.

The library was packaged and uploaded to the Python package registry, including the scripts that are used to run the experiments here. You can download it directly with {\tt pip install DeepGProp}. It has been released under the GNU GPL v3, and between releases it's also available from its GitHub repository \url{https://github.com/lulivi/dgp-lib}.

Since we have implemented {\sf EvoMLP} using the Python programming language. Tests with pypy, an accelerated, just-in-time version of the regular interpreter were performed, but although it is supposed to accelerate most numerical workloads, it actually was almost twice as slow as the one using the regular interpreter.
% Do we want to talk about technical implementation details as suggested in
% #10? Explaining all the code and that stuff - Luis
% In the abstract the bringing to modern languages and tools is highlighted
% Here could be a good place to present and justify these implementation details - M
% But what more detail could we present? JJ

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:res}

Initial results comparing EvoMLP with G-Prop under different circumstances were already presented in \cite{deep-g-prop}. In general, allowing the evolution to optimize the number of layers allowed to obtain significantly better results than the original results published by G-Prop. Since then, however, new datasets have been published and we will need to establish a new baseline, as well as characterize the size of problems that are suitable for optimization with this package.

In this paper we will first try to show the performance of the algorithm for this kind of problem, as well as create a baseline for comparison as we evolve the framework and the underlying algorithm. For that purpose, we have chosen {\sf spambase} as the dataset for all experiments. Taken from the UCI \cite{uci} database, it is a highly unbalanced dataset, with a good amount of features, and has been used extensively for benchmarking frameworks and algorithms. In papers such as the one by Duriqi et al. \cite{duriqi2016comparative}, which tests the algorithms implemented in WEKA, the accuracy reported is around 90\%, although it's not clear how the data set was split and if it is measured with a part of the dataset unseen before in any way. In order to measure generalization, in our experiments we have followed the PROBEN1 convention \cite{Prechelt94c} which is equivalent to a three-fold cross validation, splitting the dataset in three parts and sequentially using two of them for training and validation, finally testing them with the third part. By testing the models with an unseen part of the dataset, we guarantee that we are measuring its generalization capability, and not overfitting the model to the specific data it has been trained with.
%
\begin{table*}[h!tb]
\centering
<<spambase.table,echo=FALSE, results="asis">>=
spambase1.base <- read.csv("data/spambase1-p64.csv")

spambase <- data.frame( partition = "spambase1",
                       best.validation = min(spambase1.base$Validation),
                       median.validation = median(spambase1.base$Validation),
                       average.validation = mean(spambase1.base$Validation),
                       best.test = min(spambase1.base$Test),
                       median.test = median(spambase1.base$Test),
                       average.test = mean(spambase1.base$Test),
                       sd.test = sd(spambase1.base$Test))

comparison <- data.frame( partition = "spambase1 Theano",
                       best.validation = min(spambase1.base$Validation),
                       median.validation = median(spambase1.base$Validation),
                       best.test = min(spambase1.base$Test),
                       median.test = median(spambase1.base$Test),
                       sd.test = sd(spambase1.base$Test))

spambase2.base <- read.csv("data/spambase2-p64.csv")

spambase <- rbind( spambase,
                  data.frame( partition = "spambase2",
                       best.validation = min(spambase2.base$Validation),
                       median.validation = median(spambase2.base$Validation),
                       average.validation = mean(spambase2.base$Validation),
                       best.test = min(spambase2.base$Test),
                       median.test = median(spambase2.base$Test),
                       average.test = mean(spambase2.base$Test),
                       sd.test = sd(spambase2.base$Test) ) )

spambase3.base <- read.csv("data/spambase3-p64.csv")

spambase <- rbind( spambase,
                  data.frame( partition = "spambase3",
                       best.validation = min(spambase3.base$Validation),
                       median.validation = median(spambase3.base$Validation),
                       average.validation = mean(spambase3.base$Validation),
                       best.test = min(spambase3.base$Test),
                       median.test = median(spambase3.base$Test),
                       average.test = mean(spambase3.base$Test),
                       sd.test = sd(spambase1.base$Test) ) )

options(knitr.table.format = "latex")
knitr::kable(spambase, col.names=c("Partition","Validation,best","Validation, median","Validation,mean", "Test, best", "Test, median", "Test, average", "Test, SD"))
@
\vspace*{1mm}
\caption{Baseline error percentage results for {\sf spambase} per partition, with validation (used by the algorithm) and test (unseen data, used for generalization) results.}
\label{tab:spambase:base}
\end{table*}

We will use default parameters, except for the population (64) and the maximum number of generations (32), which have been found by exploration; we try to use a reasonable amount of exploitation with exploration that is more intense for weights, and less intense for the other possibilities of change. This version will use Python 3.8.6, as well as Theano 2.3.1 with a Theano backend; Theano is a good choice, and it was proved to be slightly faster than other backends, like TensorFlow. Results are shown in \autoref{tab:spambase:base}. That table represents the average over {\em all} individuals in the last generation. We should take into account the uncertainty in the evaluation of the fitness via training and testing, so the concept of {\em best} individual in the last generation includes the same amount of uncertainty; this is why in the table we include the median and the average; in these uncertain environments, median might be a better measure of success reached. This should be essentially considered a baseline value, and we do not want to present it as the state of the art. Looking at published results, these are comparable with those obtained with the Naive Bayes algorithm and shown, for instance, in \cite{duriqi2016comparative}, although the way of measuring them is totally different and we cannot really draw a comparison.

The time needed to reach these results is around 2 hours for every run. This shows the relative lack of performance of Python, but at the same time, 2 hours is acceptable if we do not need a real time flow of results. The spambase problem is mid-size, with many inputs and a sizable dataset, so this might be, in fact, in the limit of the kind of problems that can be approached with this algorithm, unless some improvements in performance (and accuracy) are made. Taking into account that all we have used is 2048 evaluations, it is not such a bad result, leading us to think that increasing the budget would probably obtain better results.
%
\begin{table*}[h!tb]
\centering
<<comparison.table,echo=FALSE, results="asis">>=
spambase1.tf <- read.csv("data/cec-2021-spambase-1.csv")
spambase2.tf <- read.csv("data/cec-2021-spambase-2.csv")
spambase3.tf <- read.csv("data/cec-2021-spambase-3.csv")

comparison <- rbind(comparison,
                    data.frame( partition = "spambase1 TF",
                       best.validation = min(spambase1.tf$Validation),
                       median.validation = median(spambase1.tf$Validation),
                       best.test = min(spambase1.tf$Test),
                       median.test = median(spambase1.tf$Test),
                       sd.test = sd(spambase1.tf$Test)))
                    
comparison <- rbind( comparison,
                    data.frame( partition = "spambase2 Theano",
                       best.validation = min(spambase2.base$Validation),
                       median.validation = median(spambase2.base$Validation),
                       best.test = min(spambase2.base$Test),
                       median.test = median(spambase2.base$Test),
                       sd.test = sd(spambase2.base$Test) ) )                    

comparison <- rbind( comparison,
                      data.frame( partition = "spambase2 TF",
                       best.validation = min(spambase2.tf$Validation),
                       median.validation = median(spambase2.tf$Validation),
                       best.test = min(spambase2.tf$Test),
                       median.test = median(spambase2.tf$Test),
                       sd.test = sd(spambase2.tf$Test) ) )

comparison <- rbind( comparison,
                  data.frame( partition = "spambase3 Theano",
                       best.validation = min(spambase3.base$Validation),
                       median.validation = median(spambase3.base$Validation),
                       best.test = min(spambase3.base$Test),
                       median.test = median(spambase3.base$Test),
                       sd.test = sd(spambase1.base$Test) ) )

comparison <- rbind( comparison,
                  data.frame( partition = "spambase3 TF",
                       best.validation = min(spambase3.tf$Validation),
                       median.validation = median(spambase3.tf$Validation),
                       best.test = min(spambase3.tf$Test),
                       median.test = median(spambase3.tf$Test),
                       sd.test = sd(spambase1.tf$Test) ) )
options(knitr.table.format = "latex")
knitr::kable(comparison, col.names=c("Partition","Validation,best","Validation, median", "Test, best", "Test, median", "Test, SD"))
@
\vspace*{1mm}
\caption{Comparison of baseline (Theano-based) and the last version, based on TensorFlow, error percentage results for {\sf spambase} per partition, with validation (used by the algorithm) and test (unseen data, used for generalization) results.}
\label{tab:spambase:comparison}
\end{table*}

However, another possible way of obtaining better results in one way or another is upgrading the dependencies. As a matter of fact, Keras underwent a change in the last version, being incorporated into the TensorFlow module, instead of being an independent one. That also had as a result eliminating the possibility of using different backends, reducing itself to only one backend: TensorFlow. We need to find out how this will affect the results of the algorithm, so we created a fork of the problem, changing all dependencies from the original version to the one included with TensorFlow, and run again using the same datasets and the same command line. The result is shown in \autoref{tab:spambase:comparison}. The set of last-generation results have been compared using Wilcoxon test, and in all cases difference is significant. However, as you can see, the result with this new version are worse than the original {\em in median}, although, since the standard deviation is higher, the best values are, in general, better. We should emphasize that the library code is exactly the same, except for a line that imports Keras as part of the TensorFlow library.

\begin{figure}[h!tb]
\centering
<<comparison.layers,echo=FALSE,message=FALSE>>=
spambase1.lr11 <- read.csv("data/cec-2021-spambase1-lr11.csv")
spambase2.lr11 <- read.csv("data/cec-2021-spambase2-lr11.csv")
spambase3.lr11 <- read.csv("data/cec-2021-spambase3-lr11.csv")
comparison.layers <- data.frame(experiment=c(rep("spambase1.3L",length(spambase1.tf$Test)),
                                             rep("spambase1.1L",length(spambase1.lr11$Test)),
                                             rep("spambase2.3L",length(spambase2.tf$Test)),
                                             rep("spambase2.1L",length(spambase2.lr11$Test)),
                                             rep("spambase3.3L",length(spambase3.tf$Test)),
                                             rep("spambase3.1L",length(spambase3.lr11$Test))),
                                Accuracy=c(spambase1.tf$Test,spambase1.lr11$Test,
                                           spambase2.tf$Test,spambase2.lr11$Test,
                                           spambase3.tf$Test,spambase3.lr11$Test))
ggplot(comparison.layers,aes(x=experiment,y=Accuracy,group=experiment))+geom_boxplot(notch=T)+theme_tufte()
@
\caption{Comparing test results over the three partitions allowing up to three layers to evolve (results reported above) or fixing the number of hidden layers to 1 (1L).}
\label{fig:comparison:layers}
\end{figure}
%
Once it is clear that the latest version of TensorFlow offers the best results, we will use it exclusively to check if one of the changes of EvoMLP, being able to evolve the number of layers, really brings any kind of improvement to the overall performance. On one hand, theoretically better results can be obtained; on the other hand, the search space again blows up, so with the same budget you might be, as it were, fishing in a bigger pond with a smaller fishing rod. So let us run an experiment where the number of layers is stuck at one, comparing it with these results. These are shown in \autoref{fig:comparison:layers}.

Although the difference is significant in all three cases, the problem is that it differs on which one is the best; using a single partition would probably hide that. The single-layer version is better in spambase2, the 3-layer version is better in the other two. Overall, however, there is no significant difference between in these three experiments the way we have done it, which probably means that, since we are using the same budget, as we have indicated above, the constrain added of having a single layer makes search in the space of architectures reach further, so we would need more evaluations to really see a difference. We should also take into account that we are using all the individuals in the last generation, due to the practical impossibility to select the absolute best, since the evaluation for every individual is affected by the uncertainty in training. The best results seem to indicate that, in some cases, using 3 layers is able to reach a level of accuracy that are just not possible with one layer. However, this is left as an hypothesis to be checked in the future. Besides, since training time is mainly dependent on the number of weights and not the number of layers, the time they take is virtually the same. So, eventually, evolving one or several layers is simply a judgment call that can be done for a number of reasons, including how different representations are done in several hidden layers; performance-wise, however, there is no difference.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and future work}
\label{sec:conclus}

We have introduced EvoMLP, a library released to the Python ecosystem as {\sf DeepGProp}, a new Python library that uses state-of-the-art libraries for evolutionary algorithms and neural nets, and combines them in a way that can easily design a multilayer perceptron that solves problems up to mid size, such as {\sf spambase}.

Depending on other libraries is always an issue in stochastic algorithms, but this implies that careful tests must be performed to see how that change affects our specific library. In this case, upgrading to Keras 2.4.0 has yielded higher uncertainty in the results obtained by our algorithm, although this has resulted in better exploration and eventually a better {\em best} result. Every change needs to be tested, though, because although new versions might bring performance improvements, these might result in changes in the algorithmic results.

In this paper we have proved also that this new library allows us to explore easily the possible space of search configurations, with one or several layers, and produces output that can be easily processed to obtain statistical results. This eventually has led to the conclusion that, overall, it is probably enough with evolving MLPs with a single layer as G-Prop did. However, this is again dependent on the fact that we need a better way to evaluate the results than simply using a single evaluation of all individuals in the last generation. To do a proper evaluation, we would need to perform several trainings on the stored initial weights, at least 15, and use all 15 validation results as the fitness, and then compare them statistically as proposed in \cite{wilcoxon:ga}. This is beyond the simple, proof-of-concept approach that we have had in this paper.

Taking into account the double handicap of a low budget and also the uncertainty in fitness, it might be the case that the neural net training algorithm we use is actually responsible for most of the improvement in fitness. Although the generational plot shows that average and best results improve with time, we might need many more generations to get an improvement, and that one might probably be only marginal; since we don't really know what is the optimal value for this dataset, we might have reached that limit, which would explain the small differences obtained with set of varying number of layers.

This is why as a future line of work, we will try to obtain the best results possible by finding the optimal population as well as number of generations. Since this will increase the time needed to train, additional changes will have to be included, possibly sampling the testing set in every generation, which will have the effect of increasing uncertainty, but certainly reducing training time. Sensitivity to changes in the parameters beyond population and number of generations will also have to be investigated. Finally, all possibilities of increasing speed like parallelization or using GPU instructions will also be investigated.

Finding different ways to take uncertainty into account will also be considered, from the simplest one, which is simply re-evaluating the individuals in the last generation and choose only the one that is significantly better than the rest, to introducing it further in the selection process by resampling or, as was shown in \cite{wilcoxon:ga}, take into account all fitness measures taken through all generations to select individuals.

Our commitment to open science is complete, and this paper has been developed in an open repository, which includes this paper, data obtained in the experiments, code (although it's an initial version, and it was eventually spun off to its own repository), and processed data. Check it out at \url{https://github.com/JJ/2021-cec-deep-g-prop/blob/main/README.md}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}

This paper has been supported in part by project DeepBio (TIN2017-85727-C4-2-P).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\bibliography{geneura,deep-g-prop,gprop,gpropnpl}

\end{document}
